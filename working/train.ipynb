{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc8706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa141b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env SM_FRAMEWORK=tf.keras\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile as tiff\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as albu\n",
    "from sklearn.model_selection import train_test_split, KFold, GroupKFold\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import *\n",
    "import segmentation_models as sm\n",
    "from segmentation_models import Unet, FPN\n",
    "from segmentation_models.losses import bce_jaccard_loss\n",
    "print(\"tensorflow version:\", tf.__version__)\n",
    "gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpu_devices:\n",
    "    for gpu_device in gpu_devices:\n",
    "        print(\"device available:\", gpu_device)\n",
    "policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb76e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = \"v12\"\n",
    "PARAMS = {\n",
    "    \"version\": VER,\n",
    "    \"folds\": 2,\n",
    "    \"img_size\": 256,\n",
    "    \"resize\": 4,\n",
    "    \"batch_size\" : 32,\n",
    "    \"epochs\": 2,\n",
    "    \"patience\": 20,\n",
    "    \"backbone\": \"efficientnetb0\",# efficientnetbX, resnet34/50, resnext50, seresnet34, seresnext\n",
    "    \"loss\": \"bce_dice\",\n",
    "    \"mirror\": False,\n",
    "    \"umodel\" : \"unet\",\n",
    "    \"bce_weight\": 1.,\n",
    "    \"shift\": True,\n",
    "    \"pseudo\":\"\",\n",
    "    \"lr\": 0.0002,\n",
    "    \"split\": \"kfold\",\n",
    "    \"seed\": None\n",
    "}\n",
    "DATA_PATH = \"../input/hubmap-kidney-segmentation\"\n",
    "\n",
    "\n",
    "IMGS_PATH = f\"{DATA_PATH}/tiles_r\"\n",
    "MSKS_PATH = f\"{DATA_PATH}/masks_r\"\n",
    "\n",
    "MDLS_PATH = f\"./models_{VER}\"\n",
    "if not os.path.exists(MDLS_PATH):\n",
    "    os.mkdir(MDLS_PATH)\n",
    "with open(f\"{MDLS_PATH}/params.json\", \"w\") as file:\n",
    "    json.dump(PARAMS,file)\n",
    "        \n",
    "\n",
    "if not PARAMS[\"mirror\"]:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    STRATEGY = tf.distribute.get_strategy()\n",
    "else:\n",
    "    STRATEGY = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_masks = pd.read_csv(f\"{DATA_PATH}/train.csv\").set_index(\"id\")\n",
    "df_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc2mask(encs, shape):\n",
    "    img = np.zeros(shape[0] * shape[1], dtype = np.uint8)\n",
    "    for m, enc in enumerate(encs):\n",
    "        if isinstance(enc, np.float) and np.isnan(enc): continue\n",
    "        s = enc.split()\n",
    "        for i in range(len(s) // 2):\n",
    "            start = int(s[2 * i]) - 1\n",
    "            length = int(s[2 * i + 1])\n",
    "            img[start : start + length] = 1 + m\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "def show_img_n_mask(df, img_num, resize):\n",
    "    img = tiff.imread(os.path.join(f\"{DATA_PATH}/train\", df.index[img_num] + \".tiff\"))\n",
    "    if len(img.shape) == 5: img = np.transpose(img.squeeze(), (1, 2, 0))\n",
    "    mask = enc2mask(df.iloc[img_num], (img.shape[1], img.shape[0]))\n",
    "    print(img.shape, mask.shape)\n",
    "    img = cv2.resize(img,\n",
    "                     (img.shape[1] // resize, img.shape[0] // resize),\n",
    "                     interpolation = cv2.INTER_AREA)\n",
    "    mask = cv2.resize(mask,\n",
    "                      (mask.shape[1] // resize, mask.shape[0] // resize),\n",
    "                      interpolation = cv2.INTER_NEAREST)\n",
    "    plt.figure(figsize = (8, 8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img)\n",
    "    plt.imshow(mask, alpha = 0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-literature",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenKid(Sequence):\n",
    "    \n",
    "    def __init__(self, imgs_path, msks_path, imgs_idxs, img_size, \n",
    "                 batch_size = 32, mode = \"fit\", shuffle = False,\n",
    "                 resize = None):\n",
    "        self.imgs_path = imgs_path\n",
    "        self.msks_path = msks_path\n",
    "        self.imgs_idxs = imgs_idxs\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        self.shuffle = shuffle\n",
    "        self.resize = resize\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.imgs_idxs) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.imgs_idxs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        batch_size = min(self.batch_size, len(self.imgs_idxs) - index * self.batch_size)\n",
    "        X = np.zeros((batch_size, self.img_size, self.img_size, 3), dtype = np.float32)\n",
    "        imgs_batch = self.imgs_idxs[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        if self.mode == \"fit\":\n",
    "            y = np.zeros((batch_size, self.img_size, self.img_size), dtype = np.float32)\n",
    "            for i, img_idx in enumerate(imgs_batch):\n",
    "                X[i, ], y[i] = self.get_tile(img_idx)\n",
    "            return X, y\n",
    "        elif self.mode == \"predict\":\n",
    "            for i, img_idx in enumerate(imgs_batch):\n",
    "                X[i, ] = self.get_tile(img_idx)\n",
    "            return X\n",
    "        else:\n",
    "            raise AttributeError(\"Fit mode parameter error\")\n",
    "            \n",
    "    def get_tile(self, img_idx):\n",
    "        img_path = f\"{self.imgs_path}/{img_idx}.png\"\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"error loading image: {image_path}\")\n",
    "        img = img.astype(np.float32) / 255\n",
    "        if self.mode == \"fit\":\n",
    "            msk_path = f\"{self.msks_path}/{img_idx}.png\"\n",
    "            msk = cv2.imread(msk_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if msk is None:\n",
    "                print(f\"error loading mask: {msk_path}\")\n",
    "            msk = msk.astype(np.float32)\n",
    "            return img, msk\n",
    "        else:\n",
    "            return img\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_idxs = [x.replace(\".png\", \"\") for x in os.listdir(IMGS_PATH) if \".png\" in x]\n",
    "train_datagen = DataGenKid(\n",
    "        imgs_path = IMGS_PATH,\n",
    "        msks_path = MSKS_PATH,\n",
    "        imgs_idxs = imgs_idxs,\n",
    "        img_size = PARAMS[\"img_size\"],\n",
    "        batch_size = PARAMS[\"batch_size\"],\n",
    "        mode = \"fit\",\n",
    "        shuffle = True,\n",
    "        resize = None\n",
    ")\n",
    "\n",
    "val_datagen = DataGenKid(\n",
    "        imgs_path = IMGS_PATH,\n",
    "        msks_path = MSKS_PATH,\n",
    "        imgs_idxs = imgs_idxs,\n",
    "        img_size = PARAMS[\"img_size\"],\n",
    "        batch_size = PARAMS[\"batch_size\"],\n",
    "        mode = \"fit\",\n",
    "        shuffle = False,\n",
    "        resize = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf8b19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_item(item_nr):\n",
    "    bsize = min(8, PARAMS[\"batch_size\"])\n",
    "    Xt, yt = train_datagen.__getitem__(item_nr)\n",
    "    print(\"test X: \", Xt.shape)\n",
    "    print(\"test y: \", yt.shape)\n",
    "    fig, axes = plt.subplots(figsize = (16, 4), nrows = 2, ncols = bsize)\n",
    "    for j in range(bsize):\n",
    "        axes[0, j].imshow(Xt[j])\n",
    "        axes[0, j].set_title(j)\n",
    "        axes[0, j].axis(\"off\")\n",
    "        axes[1, j].imshow(yt[j])\n",
    "        axes[1, j].axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c65225",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_item(223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth = 1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2 * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth = 1):\n",
    "    return (1 - dice_coef(y_true, y_pred, smooth))\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return PARAMS[\"bce_weight\"] * binary_crossentropy(y_true, y_pred) + \\\n",
    "        (1 - PARAMS[\"bce_weight\"]) * dice_loss(y_true, y_pred)\n",
    "\n",
    "def get_model(backbone, input_shape, loss_type = \"bce_dice\",\n",
    "              umodel = \"unet\", classes = 1, lr = .001):\n",
    "    with STRATEGY.scope():\n",
    "        if loss_type == \"bce_dice\":\n",
    "            loss = bce_dice_loss\n",
    "        \n",
    "        # TODO: Implement other bce_jaccard_loss\n",
    "            \n",
    "        else: \n",
    "            raise AttributeError(\"loss mode parameter error\")\n",
    "        \n",
    "        if umodel == \"unet\":\n",
    "            model = Unet(backbone_name = backbone, encoder_weights = \"imagenet\",\n",
    "                         input_shape = input_shape,\n",
    "                         classes = classes, activation = \"sigmoid\")\n",
    "            \n",
    "        elif umodel == \"fpn\":\n",
    "            model = FPN(backbone_name = backbone, encoder_weights = \"imagenet\",\n",
    "                        input_shape = input_shape,\n",
    "                        classes = classes, activation = \"sigmoid\")\n",
    "        \n",
    "        else:\n",
    "            raise AttributeError(\"umodel mode parameter error\")\n",
    "            \n",
    "        model.compile(\n",
    "            optimizer = tfa.optimizers.Lookahead(\n",
    "                tf.keras.optimizers.Adam(learning_rate = lr),\n",
    "                sync_period = max(6, int(PARAMS[\"patience\"] / 4))\n",
    "            ),\n",
    "            loss = loss,\n",
    "            metrics = [dice_coef]\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-certificate",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(mparams, n_fold, train_datagen, val_datagen):\n",
    "    model = get_model(\n",
    "        mparams[\"backbone\"],\n",
    "        input_shape = (mparams[\"img_size\"], mparams[\"img_size\"], 3),\n",
    "        loss_type = mparams[\"loss\"],\n",
    "        umodel = mparams[\"umodel\"],\n",
    "        lr = mparams[\"lr\"]\n",
    "    )\n",
    "    checkpoint_path = f\"{MDLS_PATH}/model_{n_fold}.hdf5\"\n",
    "    earlystopper = EarlyStopping (\n",
    "        monitor = \"val_dice_coef\",\n",
    "        patience = mparams[\"patience\"],\n",
    "        verbose = 0,\n",
    "        restore_best_weights = True,\n",
    "        mode = \"max\"\n",
    "    )\n",
    "    lrreducer = ReduceLROnPlateau(\n",
    "        monitor = \"val_dice_coef\",\n",
    "        factor = .1,\n",
    "        patience = int(mparams[\"patience\"] / 2),\n",
    "        verbose = 0,\n",
    "        min_lr = 1e-7,\n",
    "        mode = \"max\"\n",
    "    )\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor = \"val_dice_coef\",\n",
    "        verbose = 0,\n",
    "        save_best_only = True,\n",
    "        save_weights_only = True,\n",
    "        mode = \"max\"\n",
    "    )\n",
    "    callbacks = [earlystopper, checkpointer]\n",
    "    callbacks.append(lrreducer)\n",
    "    print(\"lr reduce on plateau\")\n",
    "    history = model.fit(\n",
    "        train_datagen,\n",
    "        validation_data = val_datagen,\n",
    "        callbacks = callbacks,\n",
    "        epochs = mparams[\"epochs\"],\n",
    "        verbose = 1\n",
    "    )\n",
    "    history_file = f\"{MDLS_PATH}/history_{n_fold}.json\"\n",
    "    dict_to_save = {}\n",
    "    for k, v in history.history.items():\n",
    "        dict_to_save.update({k: [np.format_float_positional(x) for x in history.history[k]]})\n",
    "    with open(history_file, \"w\") as file:\n",
    "        json.dump(dict_to_save, file)\n",
    "    model.load_weights(checkpoint_path)\n",
    "    return model, history  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddfb1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for iname in list(set([x[:9] for x in imgs_idxs])):\n",
    "    print(\"img name:\", iname,\n",
    "          \"| imgs number:\", len([x for x in imgs_idxs if x[:9] == iname]))\n",
    "if PARAMS[\"split\"] == \"kfold\":\n",
    "    kfold = KFold(n_splits = PARAMS[\"folds\"],\n",
    "                  random_state = PARAMS[\"seed\"],\n",
    "                  shuffle = True).split(imgs_idxs)\n",
    "else:\n",
    "    raise AttributeError(\"split mode parameter error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cedee91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epoch_by_folds = []\n",
    "loss_by_folds = []\n",
    "dice_coef_by_folds = []\n",
    "\n",
    "for n, (tr, te) in enumerate(kfold):\n",
    "    print(\"=\" * 10, f\"FOLD {n}\", \"=\" * 10)\n",
    "    X_tr = [imgs_idxs[i] for i in tr]; X_val = [imgs_idxs[i] for i in te]\n",
    "    print(\"train:\", len(X_tr), \"| test:\", len(X_val))\n",
    "    print(\"groups train:\", set([x[:9] for x in X_tr]),\n",
    "          \"\\ngroups test:\", set([x[:9] for x in X_val]))\n",
    "    train_datagen = DataGenKid(\n",
    "        imgs_path = IMGS_PATH,\n",
    "        msks_path = MSKS_PATH,\n",
    "        imgs_idxs = X_tr,\n",
    "        img_size = PARAMS[\"img_size\"],\n",
    "        batch_size = PARAMS[\"batch_size\"],\n",
    "        mode = \"fit\",\n",
    "        shuffle = True,\n",
    "        resize = None\n",
    "    )\n",
    "    \n",
    "    val_datagen = DataGenKid(\n",
    "        imgs_path = IMGS_PATH,\n",
    "        msks_path = MSKS_PATH,\n",
    "        imgs_idxs = X_val,\n",
    "        img_size = PARAMS[\"img_size\"],\n",
    "        batch_size = PARAMS[\"batch_size\"],\n",
    "        mode = \"fit\",\n",
    "        shuffle = False,\n",
    "        resize = None\n",
    "    )\n",
    "    model, history = train_model(PARAMS, n, train_datagen, val_datagen)\n",
    "    \n",
    "    plt.plot(history.history[\"loss\"], label = \"loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label = \"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(history.history[\"dice_coef\"], label = \"dice_coef\")\n",
    "    plt.plot(history.history[\"val_dice_coef\"], label = \"val_dice_coef\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    best_epoch = np.argmax(history.history[\"val_dice_coef\"])\n",
    "    best_loss = history.history[\"val_loss\"][best_epoch]\n",
    "    best_dice_coef = history.history[\"val_dice_coef\"][best_epoch]\n",
    "    print(\"best epoch:\", best_epoch,\n",
    "         \"| best loss:\", best_loss,\n",
    "         \"| best dice coef:\", best_dice_coef)\n",
    "    epoch_by_folds.append(best_epoch)\n",
    "    loss_by_folds.append(best_loss)\n",
    "    dice_coef_by_folds.append(best_dice_coef)\n",
    "    del train_datagen, val_datagen, model; gc.collect()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = PARAMS.copy()\n",
    "result[\"bavg_epoch\"] = np.mean(epoch_by_folds)\n",
    "result[\"bavg_loss\"] = np.mean(loss_by_folds)\n",
    "result[\"bavg_dice_coef\"] = np.mean(dice_coef_by_folds)\n",
    "result[\"dice_by_folds\"] = \" \".join([f\"{x:.4f}\" for x in dice_coef_by_folds])\n",
    "with open(f\"{MDLS_PATH}/params.json\", \"w\") as file:\n",
    "    json.dump(result, file)\n",
    "if not os.path.exists(\"results.csv\"):\n",
    "    df_save = pd.DataFrame(result, index = [0])\n",
    "    df_save.to_csv(\"results.csv\", sep = \"\\t\")\n",
    "else:\n",
    "    df_old = pd.read_csv(\"results.csv\", sep = \"\\t\", index_col = 0)\n",
    "    df_save = pd.DataFrame(result, index = [df_old.index.max() + 1])\n",
    "    df_save = df_old.append(df_save, ignore_index = True)\n",
    "    df_save.to_csv(\"results.csv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"results.csv\", sep = \"\\t\", index_col=0).iloc[:,4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-diameter",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "larger = 4\n",
    "test_models = []\n",
    "for n_fold in list(range(PARAMS[\"folds\"])):\n",
    "    \n",
    "    checkpoint_path = f\"{MDLS_PATH}/model_{n_fold}.hdf5\"\n",
    "    print(checkpoint_path)\n",
    "    model_lrg = get_model (\n",
    "        PARAMS[\"backbone\"],\n",
    "        input_shape = (PARAMS[\"img_size\"] * larger, PARAMS[\"img_size\"] * larger, 3),\n",
    "        loss_type = PARAMS[\"loss\"],\n",
    "        umodel = PARAMS[\"umodel\"]\n",
    "    )\n",
    "    model_lrg.load_weights(checkpoint_path) # or .set_weights(model.get_weights()) from smaller model\n",
    "    test_models.append(model_lrg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5079e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_num = 0\n",
    "resize = PARAMS[\"resize\"]\n",
    "shft = 0.6\n",
    "wnd = PARAMS[\"img_size\"] * larger\n",
    "img = tiff.imread(os.path.join(f\"{DATA_PATH}/train\", df_masks.index[img_num] + \".tiff\"))\n",
    "if len(img.shape) == 5: img = np.transpose(img.squeeze(), (1, 2, 0))\n",
    "mask = enc2mask(df_masks.iloc[img_num], (img.shape[1], img.shape[0]))\n",
    "print(img.shape, mask.shape)\n",
    "img = cv2.resize(img,\n",
    "                 (img.shape[1] // resize, img.shape[0] // resize),\n",
    "                 interpolation = cv2.INTER_AREA)\n",
    "mask = cv2.resize(mask,\n",
    "                  (mask.shape[1] // resize, mask.shape[0] // resize),\n",
    "                  interpolation = cv2.INTER_NEAREST)\n",
    "img = img[int(img.shape[0] * shft) : int(img.shape[0] * shft) + wnd,\n",
    "          int(img.shape[1] * shft) : int(img.shape[1] * shft) + wnd,\n",
    "          :]\n",
    "mask = mask[int(mask.shape[0] * shft) : int(mask.shape[0] * shft) + wnd,\n",
    "            int(mask.shape[1] * shft) : int(mask.shape[1] * shft) + wnd]\n",
    "plt.figure(figsize = (4, 4))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img)\n",
    "print(img.shape)\n",
    "plt.imshow(mask, alpha = 0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d992814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_np(pred, true, k = 1):\n",
    "    intersection = np.sum(pred[true == k]) * 2\n",
    "    dice = intersection / (np.sum(pred) + np.sum(true))\n",
    "    return dice\n",
    "\n",
    "def get_dice(mask, mask_lrg, th):\n",
    "    mask_pred = np.squeeze(mask_lrg > th).astype(int)\n",
    "    return dice_np(mask, mask_pred)\n",
    "\n",
    "def get_best_th_dice(mask, mask_lrg, n=100, plot = False):\n",
    "    thresholds = np.linspace(0, 1, n)\n",
    "    dices = [get_dice(mask, mask_lrg, th) for th in thresholds]\n",
    "    n_max = np.argmax(dices)\n",
    "    if plot:\n",
    "        plt.plot(thresholds, dices)\n",
    "        plt.title(f\"th: {thresholds[n_max]:.2f} dice: {dices[n_max]:.2f}\")\n",
    "        plt.show()\n",
    "    return thresholds[n_max], dices[n_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ac0a9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize = (16, 4), nrows = 1, ncols = len(test_models))\n",
    "for j in range(len(test_models)):\n",
    "    mask_lrg = test_models[j].predict(img[np.newaxis, ] / 255)\n",
    "    axes[j].imshow(np.squeeze(mask_lrg))\n",
    "    axes[j].set_title(f\"img {j}: {np.min(mask_lrg):.2f}-{np.max(mask_lrg):.2f}\")\n",
    "    axes[j].axis(\"off\")\n",
    "    print(get_best_th_dice(mask, mask_lrg))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7fcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_th_dice(mask, mask_lrg, n = 100, plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b09bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 4))\n",
    "plt.hist(mask_lrg.flatten(), bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c381269",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 4))\n",
    "plt.hist(np.where(mask_lrg < 10e-4, np.nan, mask_lrg).flatten(), bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98866d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
